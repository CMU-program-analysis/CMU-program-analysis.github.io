\documentclass[11pt]{article}
\usepackage{../../tex/math-cmds}
\usepackage{../../tex/analysis}
\usepackage{IEEEtrantools}


\title{Lecture Notes: Satisfiability Modulo Theories}
\author{17-355/17-665/17-819: Program Analysis (Spring 2019)\\
        Jonathan Aldrich\\
		{\tt aldrich@cs.cmu.edu}}
\date{}

\begin{document}

\maketitle


\section{Motivation and Overview}

We have now seen several techniques that generate logical formulas that must
then be solved. For example, in Hoare-style verification, we used weakest
preconditions and verification conditions to generate a formula of the form
$P \implies Q$. Usually $P$ and $Q$ have free variables $x$, e.g. $P$ could be
$x > 3$ and $Q$ could be $x > 1$. We want to prove that $P \implies Q$ no matter
what $x$ we choose, i.e. no matter what the model (an assignment from variables
to values) is. This is equivalent to saying $P \implies Q$ is \textit{valid}.
We'd like tools to check this automatically. Similarly, symbolic execution
generates sets of guards $g$ in terms of program and symbolic expressions. Such
guards also typically have free variables, such as symbolic values representing
all possible program inputs. We would like to determine if such \emph{path
  conditions} are feasible, when constructing trees of all possible program
executions; we moreover would like to identify values for those free variables,
because that can help generate test inputs that cover particular program paths.
SMT solving addresses this type of problem. Although the general goal won't be
feasible for all formulas, it is feasible for a useful subset of formulas.

As an overview, note that we begin by reducing formula validity to another
problem, that of \textit{satisfiability}. A formula $F$ with free variable $x$
is valid iff for all $x$, $F$ is true. That's the same thing as saying there is
no $x$ for which $F$ is false. But that's furthermore the same as saying there
is no $x$ for which $\lnot F$ is true. This last formulation is asking whether
$\lnot F$ is \textit{satisfiable}. It turns out to be easier to seach for a
single satisfying model (or prove there is none), then to show that a formula is
valid for all models. Many satisfiability modulo theories (SMT) solvers that do
this.

What does the ``modulo theories'' part of SMT mean? Well, strictly speaking
satisfiability is for boolean formulas: formulas that include boolean variables
as well as boolean operators such as $\land, \lor,$ and $\lnot$. They may
include quantifiers such as $\forall$ and $\exists$, as well. But if we want to
have variables over the integers or reals, and operations over numbers (e.g.
$+,>$, the types of relations we've included even in our very simple \WhileLang
language), we need a solver for a \textit{theory}, such as the theory of
Presburger arithmetic (which could prove that $2*x = x+x$), or the theory of
arrays (which could prove that assigning $x[y]$ to $3$ and then looking up
$x[y]$ yields $3$). SMT solvers include a basic satisfiability checker, and
allow that checker to communicate with specialized solvers for those theories.

We will begin by discussing the problem of basic satisfiability before moving on
to how to incorporate additional theories into the formulas. 

\section{DPLL for Boolean Satisfiability}

To build to more general SMT---satisfiability modulo theories, we begin by
discussing the problem of satisfiability. 

\subsection{Boolean satisfiability (SAT)}

The boolean satisfiability problem decides whether a conjunction of literals in
a theory is satisfiable.  The ``easiest'' theory is propositional logic;
and we refer to the problem of establishing satisfiability for formulas in
propositional logic as SAT, and a decision procedure for it as a ``SAT solver.''

We begin by transforming a formula $F$ into \emph{conjuctive normal form
  (CNF)}---i.e. a conjunction of disjunctions of positive or negative literals.
For example $(a \lor \lnot b) \land (\lnot a \lor c) \land (b \lor c)$ is a CNF
formula. If the formula is not already in CNF, we can put it into CNF by using
De Morgan's laws, the double negative law, and the distributive laws:

\[
\begin{array}{rcl}
\lnot(P \lor Q) & \iff & \lnot P \land \lnot Q \\
\lnot (P \land Q) & \iff & \lnot P \lor \lnot Q \\
\lnot \lnot P & \iff & P \\
(P \land (Q \lor R)) & \iff & ((P \land Q) \lor (P \land R)) \\
(P \lor (Q \land R)) & \iff & ((P \lor Q) \land (P \lor R)) \\
\end{array}
\]

The goal of the decision procedure is, given a formula, to say it is
satisfiable, and ideally to give an example satisfying assignment---this is
useful for test generation. A satisfying assignment maps variables to boolean
values. So, $X \lor Y$ is satisfiable, and one satisfying assignment for it is
$X \mapsto \mathtt{true}, Y \mapsto \mathtt{false}$ (there are other satisfying
assignments as well). $X \land \lnot X$, by contrast, is not satisfiable.

The Cook-Levin theorem established that boolean satisfiability (or SAT) is
NP-complete. SAT itself is in NP, in that one can check a candidate satisfying
assignment in polynomial time.  In the worst case, one can solve SAT for a given
formula by simply trying all possible assignments.  For example, given 

\[
\begin{array}{c}
\exists E . E \vDash (x \lor y \lor \lnot z) \land (\lnot x \lor \lnot y) \land (z)
\end{array}
\]

We can brute-force by conducting a backgracking search that tries all possible
assignments of \texttt{true} and \texttt{false} to $x, y, z$. In the worst case,
there are $2^n$ combinations, wher $n$ is the number of variables.

\subsection{The DPLL Algorithm}

The DPLL algorithm, named for its developers Davis, Putnam, Logemann, and
Loveland, is an efficient approach to deciding boolean satisfiability problems.
DPLL algorithm improves (substantially) on the backtracking search with two
additional innovations: \emph{unit propagation}, and \emph{pure literal
  elimination}.  

Let's illustrate by example. Consider the following formula:

\[
(b \lor c) \land (\mathbf{a}) \land (\lnot a \lor c \lor d) \land (\lnot c \lor d) \land (\lnot c \lor \lnot d \lor \lnot a) \land (b \lor d)
\]

There is one clause with just $a$ in it. This clause, like all other clauses,
has to be true for the whole formula to be true, so we must make $a$ true for
the formula to be satisfiable. We can do this whenever we have a clause with
just one literal in it, i.e. a unit clause. (Of course, if a clause has just
$\lnot b$, that tells us $b$ must be false in any satisfying assignment). In
this example, we use the \textit{unit propagation} rule to replace all
occurrences of a with true. After simplifying, this gives us:

\[
(b \lor c) \land (c \lor d) \land (\lnot c \lor d) \land (\lnot c \lor \lnot d) \land (b \lor d)
\]

Now here we can see that $b$ always occurs positively (i.e. without a $\lnot$ in
front of it within a CNF formula). If we choose $b$ to be true, that eliminates
all occurrences of $b$ from our formula, thereby making it simpler---but it
doesn't change the satisfiability of the underlying formula. An analogous
approach applies when $c$ always occurs negatively, i.e. in the form $\lnot c$.
We say that a literal that occurs only positively, or only negatively, in a
formula is \textit{pure}. Therefore, this simplification is called the
\textit{pure literal elimination} rule, and applying it to the example above
gives us:

\[
(c \lor d) \land (\lnot c \lor d) \land (\lnot c \lor \lnot d)
\]

Now for this formula, neither of the above rules applies. We just have to pick a
literal and guess its value. Let's pick $c$ and set it to true. Simplifying, we
get:

\[
(d) \land (\lnot d)
\]

After applying the unit propagation rule (setting $d$ to true) we get:

\[
(\ltrue) \land (\lfalse)
\]

which is equivalent to false, so this didn't work out. But remember, we guessed
about the value of $c$. Let's backtrack to the formula where we made that
choice:

\[
(c \lor d) \land (\lnot c \lor d) \land (\lnot c \lor \lnot d)
\]

and now we'll try things the other way, i.e. with $c = \lfalse$.  Then we get the formula

\[
(d)
\]

because the last two clauses simplified to true once we know $c$ is false. Now
unit propagation sets $d = \ltrue$ and then we have shown the formula is
satisfiable. A real DPLL algorithm would keep track of all the choices in the
satisfying assignment, and would report back that $a$ is true, $b$ is true, $c$
is false, and $d$ is true in the satisfying assignment.

This procedure---applying unit propagation and pure literal elimination eagerly,
then guessing a literal and backtracking if the guess goes wrong---is the
essence of DPLL. Here's an algorithmic statement of DPLL, adapted slightly from
a version on Wikipedia:

\begin{algorithmic}

\Function{DPLL}{$\phi$}
    \If{$\phi = \ltrue$}
        \State \Return \ltrue
    \EndIf
    \If{$\phi$ contains a \lfalse\ clause}
        \State \Return \lfalse
    \EndIf
    \ForAll{unit clauses $l$ in $\phi$}
        \State $\phi \gets$ \Call{unit-propagate}{$l, \phi$}
    \EndFor
    \ForAll{literals $l$ occurring pure in $\phi$}
        \State $\phi \gets$ \Call{pure-literal-assign}{$l, \phi$}
    \EndFor
    \State $l \gets$ \Call{choose-literal}{$\phi$}
    \State \Return \Call{DPLL}{$\phi \land l$} $\lor$ \Call{DPLL}{$\phi \land \lnot l$} 
\EndFunction

\end{algorithmic}

Mostly the algorithm above is straightforward, but there are a couple of notes.
First of all, the algorithm does unit propagation before pure literal
assignment. Why? Well, it's good to do unit propagation first, because doing so
can create additional opportunities to apply further unit propagation as well as
pure literal assignment. On the other hand, pure literal assignment will never
create unit literals that didn't exist before. This is because pure literal
assignment can eliminate entire clauses but it never makes an existing clause
shorter.

Secondly, the last line implements backtracking. We assume a short-cutting
$\lor$ operation at the level of the algorithm. So if the first recursive call
to DPLL returns true, so does the current call--but if it returns fall, we
invoke DPLL with the chosen literal negated, which effectively backtracks.

\exercise{1} Apply DPLL to the following formula, describing each step (unit
propagation, pure literal elimination, choosing a literal, or backtracking) and
showing now it affects the formula until you prove that the formula is
satisfiable or not:

\[
(a \lor b) \land (a \lor c) \land (\lnot a \lor c) \land (a \lor \lnot c) \land (\lnot a \lor \lnot c) \land (\lnot d)
\]

There is a lot more to learn about DPLL, including hueristics for how to choose
the literal l to be guessed and smarter approaches to backtracking (e.g.
non-chronological backtracking), but in this class, let's move on to consider
SMT.


\section{Solving SMT Problems}

Our approach above targets formulas in the theory of propositional logic.
However, there are many other possibly useful theories, and many useful formulas
may mix them. For example, consider a conjunction of the following
formulas:\footnote{This example is due to Oliveras and Rodriguez-Carbonell}

\[
\begin{array}{l}
f(f(x)-f(y)) = a \\
f(0) = a+2 \\
x = y \\
\end{array}
\]

This problem mixes linear arithmetic with the theory of uninterpreted functions
(here, $f$ is some unknown function). We may have a satisfiability procedure for
each theory involved in the formula, but how can we deal with their combination?
Note that we \emph{can't} in general just separate out the terms from each
theory in a formula to see if they are separately satisfiable, because the two
satisfying assignments might not be compatible.  Instead, we have to handle each
domain separately (as a theory), and then combine them all together using DPLL
and SAT as the ``glue''. 

\subsection{Definitions} 

A \emph{satisfiability modulo theories (SMT)} solver operates on propositions
involving both logical terms and terms from theories. Modern SMT solvers can use
any theory that satisfies a particular interface, descuse below. Effectively,
such solvers replace all the theory clauses in a mixed-theory formula with
special propositional variables and then use a pure SAT solver to solve the
result.  If the solution involves any of the theory clauses, the sovler then
asks the theory if they can all be true.  If not, new constraints are added to
the formula, and the process repeats.  

In general, a \emph{thoery} is a set of sentences (syntax) with a deductive
system that can determine satisfiability (semantics). Usually, the set of
sentences is formally defined by a grammar of terms over atoms. The satisfying
assignment (or model, or interpretation) maps literals (terms or negated terms)
to booleans. We have had practice expressing these kinds of systems all
semester! Useful theories include linear and non-linear arithmetic, bitvectors,
arrays, quantifiers, or strings, as well as uninterpreted functions (like $f$ in
our example above).

An important feature of the kinds of theories we are discussing is that they all
understand \emph{equality}.  We do not delve deeply into how/why in these notes,
but the fact is important to how SMT can solve formulas that mix theories, as we
will see below.  


\subsection{Basic SMT idea, illustrated}

We will work through our example above to demonstrate the ideas behind SMT. The
first step is to separate the multiple theories. We can do this by replacing
expressions with fresh variables, in a procedure named Nelson-Oppen after its
two inventors. For example, in the first formula, we'd like to factor out the
subtraction, so we generate a fresh variable and divide the formula into two:

\[
\begin{array}{ll}
f(e1) = a       & \textit{// in the theory of uninterpreted functions now} \\
e1 = f(x)-f(y)  & \textit{// still a mixed formula} \\
\end{array}
\]

Now we want to separate out $f(x)$ and $f(y)$ as variables $e2$ and $e3$, so we get:

\[
\begin{array}{ll}
e1 = e2 - e3    & \textit{// in the theory of arithmetic now} \\
e2 = f(x)       & \textit{// in the theory of uninterpreted functions} \\
e3 = f(y)       & \textit{// in the theory of uninterpreted functions} \\
\end{array}
\]

We can do the same for $f(0) = a+2$, yielding:

\[
\begin{array}{l}
f(e4) = e5 \\
e4 = 0 \\
e5 = a + 2 \\
\end{array}
\]

We now have formulas in two theories. First, formulas in the theory of
uninterpreted functions:

\[
\begin{array}{l}
f(e1) = a \\
e2 = f(x) \\
e3 = f(y) \\
f(e4) = e5 \\
x = y \\
\end{array}
\]

And second, formulas in the theory of arithmetic:

\[
\begin{array}{l}
e1 = e2 - e3 \\
e4 = 0 \\
e5 = a + 2 \\
x = y \\
\end{array}
\]

Notice that $x = y$ is in both sets of formulas. We will use the fact that
equality is smoething that every theory understands, in a moment. First,
however, For now, let's run a solver. The solver for uninterpreted functions has
a congruence closure rule that states, for all $f, x,$ and $y$, if $x = y$ then
$f(x) = f(y)$. Applying this rule (since $x=y$ is something we know), we
discover that $f(x) = f(y)$. Since $f(x) = e2$ and $f(y) = e3$, by transitivity
we know that $e2 = e3$.

But $e2$ and $e3$ are symbols that the arithmetic solver knows about, so we add
$e2=e3$ to the set of formulas we know about arithmetic. Now the arithmetic
solver can discover that $e2-e3 = 0$, and thus $e1 = e4$. We communicate this
discovered equality to the uninterpreted functions theory, and then we learn
that $a = e5$ (again, using congruence closure and transitivity).

This fact goes back to the arithmetic solver, which evaluates the following
constraints:

\[
\begin{array}{l}
e1 = e2 - e3 \\
e4 = 0 \\
e5 = a + 2 \\
x = y \\
e2 = e3 \\
a = e5 \\
\end{array}
\]

Now there is a contradiction: $a = e5$ but $e5 = a + 2$. That means the original
formula is unsatisfiable.

In this case, one theory was able to infer equality relationships that another
theory could directly use. But sometimes a theory doesn't figure out an equality
relationship, but only certain correlations, e.g., e1 is either equal to e2 or
e3. In the more general case, we can simply generate a formula that represents
all possible equalities between shared symbols, which would look something like:

\[
(e1 = e2 \lor e1 \neq e2) \land (e2 = e3 \lor e2 \neq e3) \land (e1 = e3 \lor e1 \neq e3) \land ...
\]

We can now look at all possible combinations of equalities. In fact, we can use
DPLL to do this, and DPLL also explains how we can combine expressions in the
various theories with boolean operators such as $\land$ and $\lor$. If we have a
formula such as:

\[
x \geq 0 \land y = x + 1 \land (y > 2 \lor y < 1)
\]

(note: if we had multiple theories, I am assuming we've already added the
equality constraints between them, as described above)

We can then convert each arithmetic (or uninterpreted function) formula into a
fresh propositional symbol, to get:

\[
p1 \land p2 \land (p3 \lor p4)
\]

and then we can run a SAT solver using the DPLL algorithm. DPLL will return a
satisfying assignment, such as $p1, p2, \lnot p3, p4$. We then check this
against each of the theories. In this case, the theory of arithmetic finds a
contradiction: $p1$, $p2$, and $p4$ can't all be true, because $p1$ and $p2$
together imply that $y \ge 1$. We add a clause saying that these can't all be
true and give it back to the SAT solver:

\[
p1 \land p2 \land (p3 \lor p4) \land (\lnot p1 \lor \lnot p2 \lor \lnot p3)
\]

Running DPLL again gives us $p1, p2, p3, \lnot p4$. We check this against the
theory of arithmetic, and it all works out.

\subsection{DPLL(T)} 

This combination of DPLL with a theory $T$ is called DPLL(T) or (DPLL-T).  It is
an SMT algorithm based on the DPLL SAT solver, but parameterized with respect to
a set of theories T.  
%
At a high level, DPLL(T) works as we illustrated above: it converts mixed
constraints to boolean constraints and then runs DPLL, and then checks the
resulting assignments with the underlying theories to determine if they are
valid. The version of DPLL used in DPLL(T) has two key changes compared to the
original, however.

First, DPLL(T) does not use the pure variable elimination optimization.  This is
because, in pure propositional logic, variables are necessarily independent.  If
some variable $x$ only appears positively, you can set it to \texttt{true} and
save time.  With theories, variables may be dependent.  For example, consider

\[
\begin{array}{c}
(\mathbf{x > 10} \lor x < 3) \land (\mathbf{x > 10} \lor x < 9) \land (x < 7)
\end{array}
\]

In the above, $x > 10$, but if we just set that term to be true as part of the
model, the other terms all become false.  We cannot simply skip over it. 

Second, unit propagation interacts with the theories to add constraints to the
formula when available.  We saw this example above, when the theory of
arithmetic found a contraditiction in the formula 
\[
p1 \land p2 \land (p3 \lor p4)
\]

And the solving procedure added a clause to the formula before giving it back to
the SAT solver. 

\subsection{Bonus: Arithmetic solvers}

We discussed above how the solver for the theory of uninterpreted functions
work; how does the arithmetic solver work? In cases like the above example where
we assert formulas of the form $y = x + 1$, we can eliminate $y$ by substituting
it with $x+1$ everywhere. In the cases where we only constrain a variable using
inequalities, there is a more general approach called Fourier-Motzkin
Elimination. In this approach, we take all inequalities that involve a variable
x and transform them into one of the following forms:

\[
\begin{array}{c}
A \leq x \\
x \leq B
\end{array}
\]

where $A$ and $B$ are linear formulas that don't include $x$. We can then
eliminate $x$, replacing the above formulas with the equation $A \leq B$. If we
have multiple formulas with $x$ on the left and/or right, we just conjoin the
cross product. There are various optimizations that are applied in practice, but
the basic algorithm is general and provides a broad understanding of how
arithmetic solvers work.




\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
