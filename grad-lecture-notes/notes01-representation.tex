\documentclass[11pt]{article}
\usepackage{palatino}
\usepackage{latexsym}
\usepackage{verbatim}
\usepackage{alltt}
\usepackage[margin=1.0in]{geometry}
\usepackage{amsmath,proof,amsthm,amssymb,enumerate}
\usepackage{math-cmds}

\newcommand{\definition}[2]
  {\bigskip
   \begin{tabular}{p{1.5in}p{4.0in}}
        \textbf{#1} & #2 \\
        \end{tabular}
  }

\def\Natural{\mathbb{N}}
\def\Integer{\mathbb{Z}}
\def\prop{\textsf{\,\,prop}}
\def\thm{\textsf{\,\,thm}}
\def\implies{\Rightarrow}

\def\While{\textsc{While}}
\def\WhileThAddr{\textsc{While3Addr}}

\title{Lecture Notes:\\
The \textsc{While} and \textsc{While3Addr} Language}
\author{15-819O: Program Analysis \emph{(Spring 2016)} \\
        Claire Le Goues \\
		{\tt clegoues@cs.cmu.edu}}
\date{}
\begin{document}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\maketitle

\section{The \textsc{While} Language}

In this course, we will study the theory of analyses using a simple
programming language called \While, along with various extensions.
The \While~language is at least as old as Hoare's 1969 paper on
a logic for proving program properties (to be discussed in a later lecture).
It is a simple imperative language, with (to start!) assignment to local variables,
if statements, while loops, and simple integer and boolean expressions.

We use the following metavariables to describe different
categories of syntax.  The letter on the left will be used as a variable
representing a piece of a program. On the right, we describe the kind
of program piece that variable represents:

\[
\begin{array}{ll}
S   & \mbox{statements}\\
a   & \mbox{arithmetic expressions (AExp)}\\
x,y & \mbox{program variables}\\
n   & \mbox{number literals}\\
P   & \mbox{boolean predicates (BExp)}\\
\end{array}
\]


The syntax of \While~is shown below.  Statements $S$ can be an
assignment $x := a$, a skip statement, which does nothing (similar to
a lone semicolon or open/close bracket in C or Java), and \texttt{if} and \texttt{while}
statements, whose condition is a boolean predicate $P$.  Arithmetic
expressions $a$ include variables $x$, numbers $n$, and one of several
arithmetic operators, abstractly represented by $op_a$.  Boolean
expressions include true, false, the negation of another boolean
expression, boolean operators $op_b$ applied to other boolean
expressions, and relational operators $op_r$ applied to arithmetic
expressions.

\[
\begin{array}{llllll}
S & \bnfdef & x := a & a & \bnfdef & x \\
  & \bnfalt & \mbox{skip} & & \bnfalt & n \\
  & \bnfalt & S_1;~ S_2 &  & \bnfalt & a_1 ~op_a~ a_2 \\
  & \bnfalt & \mbox{if}~ P ~\mbox{then}~ S_1 ~\mbox{else}~ S_2 \\
  & \bnfalt & \mbox{while}~ P ~\mbox{do}~ S & op_a & \bnfdef & + \bnfalt - \bnfalt * \bnfalt /\\[1ex]
\end{array}
\]
\[
\begin{array}{lll}

P & \bnfdef & \mbox{true} \\
  & \bnfalt & \mbox{false} \\
  & \bnfalt & \mbox{not}~ P \\
  & \bnfalt & P_1 ~op_b~ P_2 \\
  & \bnfalt & a_1 ~op_r~ a_2 \\[1ex]

op_b & \bnfdef & \mbox{and} \bnfalt \mbox{or} \\[1ex]

op_r & \bnfdef & <~ \bnfalt ~\le~ \bnfalt ~=~ \bnfalt ~>~ \bnfalt ~\ge~

\end{array}
\]


\section{\textsc{While3Addr}: A Representation for Analysis}

For analysis, the
source-like definition of \While~can sometimes prove inconvenient.  For example,
\While~has three separate syntactic forms---statements, arithmetic
expressions, and boolean predicates---and we would have to define the semantics
of each separately to reason about it.  A simpler and more regular representation of programs will
help simplify certain of our formalisms.

As a starting point, we will eliminate recursive arithmetic and boolean
expressions and replace them with simple atomic statement forms, which are
called \textit{instructions}, after the assembly language instructions that they
resemble.  For example, an assignment statement of the form $w = x*y+z$ will be
rewritten as a multiply instruction followed by an add instruction.  The
multiply assigns to a temporary variable $t_1$, which is then used in the
subsequent add:

\[
\begin{array}{l}
t_1 = x * y \\
w = t_1 + z
\end{array}
\]

As the translation from expressions to instructions suggests, program analysis
is typically studied using a representation of programs that is not only
simpler, but also lower-level than the source (\While, in this instance)
language.  Many Java analyses are actually conducted on byte code, for example.
Typically, high-level languages come with features that are numerous and
complex, but can be reduced into a smaller set of simpler primitives.  Working
at the lower level of abstraction thus also supports simplicity in the compiler.

Control flow constructs such as \texttt{if} and \texttt{while} are similarly translated into
simpler \texttt{goto} and \texttt{conditional branch} constructs that jump to a particular
(numbered) instruction.  For example, a statement of the form \texttt{if} $P$ \texttt{then} $S_1$
\texttt{else} $S_2$ would be translated into:

\[
\begin{array}{ll}
1: & \mbox{if}~P~\mbox{then goto}~4\\
2: & S_2\\
3: & \mbox{goto}~5\\
4: & S_1\\
5: & \textit{rest of program...}\
\end{array}
\]

\noindent \emph{How would you translate While statements?} 

%The translation of a statement of the form \texttt{while} $P$ \texttt{do} $S$ is similar:
%
%\[
%\begin{array}{ll}
%1: & \mbox{if not}~P~\mbox{goto}~4\\
%2: & S\\
%3: & \mbox{goto}~1\\
%4: & \textit{rest of program...}\
%\end{array}
%\]

\vspace{1em}

This form of code is often called 3-address code, because every instruction
has at most two source operands and one result operand.  
%
We now define the syntax for 3-address code produced from the \While~
language, which we will call \WhileThAddr.  This language consists of a
set of simple instructions that load a constant into a variable, copy from one
variable to another, compute the value of a variable from two others, or jump
(possibly conditionally) to a new address $n$.  A program $P$ is just a map from
addresses to instructions:\footnote{The idea of the mapping between numbers and instructions maps
conceptually to Nielsens' use of \emph{labels} in the \While~language
specification in the text.  This concept is akin to mapping line numbers to code.}

\[
\begin{array}{llllll}
I & \bnfdef & x := n & op & \bnfdef & + \bnfalt - \bnfalt * \bnfalt / \\
  & \bnfalt & x := y & op_r & \bnfdef & <~ \bnfalt ~=~  \\
  & \bnfalt & x := y ~op~ z & P & \in & \Natural \rightarrow I\\
  & \bnfalt & \mbox{goto}~n \\
  & \bnfalt & \mbox{if}~x~op_r~0~\mbox{goto}~n \\[1ex]
\end{array}
\]

\noindent Formally defining a translation from a source language such as \While~to
a lower-level intermediate language such as \WhileThAddr~is possible, but
 more appropriate for the scope of a compilers course.  For our purposes, the 
 examples above should suffice as intuition.  We will focus
instead on semantics and on formalizing program analyses.


\def\prop{\textsf{\,\,prop}}
\def\thm{\textsf{\,\,thm}}
\def\implies{\Rightarrow}

\newcommand{\join}{\sqcup}
\newcommand{\meet}{\sqcap}
\newcommand{\alap}{\sqsubseteq}


\section{Operational Semantics} 

To reason about the correctness of an analysis, we need a clear definition of
what a program \textit{means}.  There are many ways of giving such definitions;
the most common technique in industry is to define a language using an English
document, such as the Java Language Specification.  However, natural
language specifications, while accessible to all programmers, are often
imprecise.  This imprecision can lead to many problems, such as incorrect
or incompatible compiler implementations, but more importantly for our
purposes, analyses that give incorrect results.

A better alternative is a formal definition of program semantics.  We begin with
\textit{operational semantics}.
which mimics, at a high level, the operation of a computer
executing the program, including a program counter, values for program
variables, and (eventually) a representation of the heap.  Such a semantics also
reflects the way that techniques such as dataflow analysis or Hoare Logic reason
about the program, so it is convenient for our purposes.

There are two broad classes of operational semantics: \emph{big-step operational
  semantics}, which specifies the entire operation of a given expression or
statement; and \emph{small-step operational semantics}, which specifies the
operation of the program one step at a time.\footnote{This is just an introduction, 
and does not cover all of the ways to specify or use operational semantics; I'm covering 
enough to support a conversation about abstract interpretation and correctness.}

\subsection{\While: Big-step operational semantics}

Let's start with \While~
expressions, restricting our attention to arithmetic expressions for simplicity.
What is the meaning of a \While~expression?  Some expressions, like a natural number, have
a very clear meaning: The ``meaning'' of 5 is just, well, 5.  But what about $x
+ 5$?  The meaning of this expression clearly depends on the value of the
variable $x$.  We must \emph{abstract} the value of variables as a function from
variable names to integer values:

\[
\begin{array}{lll}

\mathcal{E} & \in & \textit{Var} \rightarrow \Integer \\[1ex]
\end{array}
\]

\noindent We use $E$ to range over $\mathcal{E}$, denoting a particular program
\emph{state}.  The meaning of an expression using a
variable, like $x + 5$, involves ``looking up'' the variable value in the associated $E$, and
substituting it in.  
%
Given a state, we can then write a \emph{judgement} as follows:

\[
\begin{array}{ccc}
<e, E> & \Downarrow & n
\end{array}
\]

\noindent This means that the expression $e$ evaluates to $n$ in state $E$.  This formulation is
called \emph{big-step} operational semantics; the $\Downarrow$ judgement
relates an expression and its ``meaning.''  
%
We then build up the meaning of more complex expressions using \emph{rules of
  inference} (also called \emph{derivation} or \emph{evaluation} rules).  An
inference rule is made up of a set of judgments above the line, known as
premises, and a judgment below the line, known as the conclusion.  The meaning
of an inference rule is that the conclusion holds if all of the premises hold:

\[
\infer{conclusion}{premise_1 & premise_2 & \ldots & premise_n}
\]

\noindent 
An inference rule with no premises is an axiom; axioms are
always true. For example, integers always evaluate
to themselves, and the meaning of a variable is its stored value in the state:

\[
\begin{array}{llr}
\infer[$\textit{ints}$]{< n, E > \Downarrow n}{}
&~~~~~~~~~~~~&
\infer[$\textit{vars}$]{<x, E> \Downarrow E(x)} {}
\end{array}
\]

\noindent Addition expressions illustrate a rule with premises: 

\[
\infer[add]{< e_1 + e_2, E > \Downarrow n_1 + n_2}{<e_1, E> \Downarrow
  n_1& <e_2, E> \Downarrow n_2}
\]

How does the value of $x$ come to be ``stored'' in $E$?  For that, we must 
consider \While~\emph{Statements}.  Unlike
expressions, statements have no direct result.  However, they can have
\emph{side effects}.  That is to say: the ``result'' or \emph{meaning} of a Statement is a
\emph{new state}.  The judgement $\Downarrow$ as applied to statements
and states therefore looks like: 

\[
< S, E> \Downarrow E' 
\]

We refer to the pair of a statement and a state ($<S, E>$) as
a \emph{configuration}; this will become important when we get to the next section.
%
We can now write rules of inference for statements, bearing in mind that
their \emph{meaning} is not an integer, but a new state.  The meaning of the
\texttt{skip} statement, for example, is an unchanged state:

\[
\infer{<\mathtt{skip}, E > \Downarrow E}{}
\]

\noindent Statement sequencing, on the other hand, does involve premises:

\[
\infer{<s_1 ; s_2, E> \Downarrow E''}{<s_1,E> \Downarrow E'
  ~~~ <s_2, E'> \Downarrow E''}
\]

\noindent The \texttt{if} statement involves two rules, one for if the
boolean predicate evaluates to \texttt{true} (rules for boolean expressions not
shown), and one for if it evaluates to \texttt{false}.  I'll show you just the
first one for demonstration:

\[
\infer{<\mathtt{if}~b~\mathtt{then}~s_1~\mathtt{else}~s_2, E> \Downarrow
  E'}{<b, E> \Downarrow \mathtt{true} & <s_1, E> \Downarrow
  E'}
\]

\noindent \emph{What should the second rule for \texttt{if} look like? Write it down!}

\vspace{1em}

\noindent This brings us to assignments, which actually update the state:

\[
\infer{<x := e, E> \Downarrow E[x := n]} {<e, E> \Downarrow n} 
\]


To fully specify the semantics of a language, one must provide a judgement rule
like this for every language construct.  Clearly, these notes only include a
subset, for brevity!

\subsection{Small-step operational semantics}

Big-step operational semantics has its uses.  Among other
nice features, it directly suggests a simple interpreter
implementation for a given language.  However, it is difficult to talk about a
statement or program whose evaluation \emph{does not terminate}.  Nor does it give us any way
to talk about intermediate states (so modeling multiple threads of control is
out).  

When we need this power, we can instead use a \emph{small-step operational
  semantics}, sometimes called \emph{structural} operational semantics.
Small-step semantics specifies program execution one step at a time.  Where
big step semantics specifies program meaning as a function between a
configuration and a new state, small step semantics models it as a step from one
program configuration to another.  

You can think of small-step semantics as a set of rules that we repeatedly apply to
configurations until we reach a \emph{final configuration} for the language ($<\mathtt{skip}, E>$,
in this case)
if ever.\footnote{Not all statements reach a final configuration, like \texttt{while true do skip}.} 
We write this new judgement using a slightly different arrow:
$\rightarrow$.  $<S, E> \rightarrow <S', E'>$ indicates one step of execution; $<S,
E> \rightarrow^* <S', E'>$ indicates zero or more steps of execution. 
%
To be complete, we should also define auxiliary small-step operators
$\rightarrow_a$ and $\rightarrow_b$ for arithmetic and boolean expressions,
respectively; only the operator for statements results in an updated state (as in big step, above).  The types are thus:

\[
\begin{array}{lll}
\rightarrow & : & (\mathtt{Stmt} \cross E) \rightarrow (\mathtt{Stmt} \cross E) \\
\rightarrow_a & : & (\mathtt{Aexp} \cross E) \rightarrow \mathtt{Aexp} \\
\rightarrow_b & : & (\mathtt{Bexp} \cross E) \rightarrow \mathtt{Bexp} \\
\end{array}
\]

\noindent We can now again write the semantics of a \While~program as rules of
inference.  Some rules look very similar to the big-step rules, just with a
different arrow.  For example, consider variables:

\[
\infer{<x, E> \rightarrow_a E(x)}{}
\]

\noindent Things get more interesting when we return to statements.  Remember,
small-step semantics express a single execution step.  So, consider an
\texttt{if} statement:

\[
\infer{<\mathtt{if}~b~\mathtt{then}~S_1~\mathtt{else}~S_2, E> \rightarrow <\mathtt{if}~b'~\mathtt{then}~S_1~\mathtt{else}~S_2, E>}
{ <b, E> \rightarrow_b~b'}
\]


\[
\infer{<\mathtt{if}~\mathtt{true}~\mathtt{then}~s_1~\mathtt{else}~s_2, E> \rightarrow
  <s_1,E'>}{}
\]

I have again omitted the \texttt{if-false} case, as an exercise to the reader; suffice
to say, there's one more \texttt{if} rule.  Regardless, contrast these rules with the
big-step rules, above.  \emph{What's the difference?}

\subsection{Small-step semantics for \WhileThAddr}

The ideas behind big- and small-step operational semantics are consistent
across languages, but the way they are written can vary based on what is
notationally convenient for a particular language or analysis.  
\WhileThAddr~is slightly different from \While, so beyond requiring different rules for its 
different constructs, it makes sense to modify our small-step notation a bit for defining the 
meaning of a \WhileThAddr~program.

First, let's revisit the \emph{configuration} to account for the slightly different \emph{meaning} of a
\WhileThAddr\ program.  As before, the configuration must include the state, which we still call $E$, mapping 
variables to values. However, a well-formed, terminating \While~program
was effectively a single statement that can be iteratively reduced to
\texttt{skip}; a \WhileThAddr~program, on the other hand, is a mapping from
natural numbers to program instructions. 
So, instead of a statement that is being reduced in steps, the \WhileThAddr~$c$ must
includes a program counter $n$, representing the next instruction to be
executed.  

Thus, a configuration $c$ of the abstract machine for \WhileThAddr~
must include the stored program $P$ (which we will generally treat implicitly),
the state environment $E$, and the current program counter $n$
representing the next instruction to be executed ($c \in E \cross \Natural$). 
%\mathbb{N}
The abstract machine executes one step at a time, executing the instruction that
the program counter points to, and updating the program counter and environment
according to the semantics of that instruction.  

This adds a tiny bit of complexity to the inference rules, because
they must explicitly consider the mapping between line number/labels and program
instructions.  We represent execution of
the abstract machine via a judgment of the form $P \vdash <E,n>
\leadsto <E',n'>$ The judgment reads: ``When executing the program $P$,
executing instruction $n$ in the state $E$ steps to a new state $E'$
and program counter $n'$.''\footnote{I could have used the same $\rightarrow$ I
  did above instead of $\leadsto$, but I don't want you to mix them up.}
To see this in action, consider a simple inference rule defining the semantics of the constant
 assignment instruction:

\[
\infer[\textit{step-const}]{P \vdash <E,n> \leadsto E[x \mapsto m],n+1}{P[n] = x := m}
\]

This states that in the case where the $n$th instruction of the program P (looked up using $P[n]$) is a constant assignment $x := m$, the abstract machine takes a step to a state in which the state $E$ is updated to map $x$ to the constant $m$, written as $E[x \mapsto m]$, and the program counter now points to the instruction at the following address $n+1$.
%
We similarly define the remaining rules:

\[
\begin{array}{c}
\infer[\textit{step-copy}]
	{P \vdash <E,n> \leadsto E[x \mapsto E[y]],n+1}
	{P[n] = x := y}\\[3ex]
	\infer[\textit{step-arith}]
	{P \vdash <E, n> \leadsto E[x \mapsto m],n+1}
	{P[n] = x := y ~op~ z & E[y] ~\mathbf{op}~ E[z] = m}\\
\end{array}
\]
\[
\begin{array}{c}	

\infer[\textit{step-goto}]
	{P \vdash <E,n> \leadsto <E,m>}
	{P[n] = \mbox{goto}~m}\\[3ex]
	
\infer[\textit{step-iftrue}]
	{P \vdash <E,n> \leadsto <E,m>}
	{P[n] = \mbox{if}~x~op_r~0~\mbox{goto}~m & E[x] ~\mathbf{op_r}~ 0 = true}\\[3ex]
	
\infer[\textit{step-iffalse}]
	{P \vdash E,n \leadsto E,n+1}
	{P[n] = \mbox{if}~x~op_r~0~\mbox{goto}~m & E[x] ~\mathbf{op_r}~ 0 = false}\\[3ex]
	
\end{array}
\]


\subsubsection*{Acknowledgements}

With grateful acknowledgement to Jonathan Aldrich for his provision of starting
materials for these notes (especially the first two sections). 

\end{document}