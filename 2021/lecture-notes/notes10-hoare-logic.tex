\documentclass[11pt]{article}
\usepackage{../../tex/math-cmds}
\usepackage{../../tex/analysis}
\usepackage{IEEEtrantools}


\title{Lecture Notes: Axiomatic Semantics and \\ Hoare-style Verification}
\author{17-355/17-665/17-819: Program Analysis (Spring 2020)\\
        Claire Le Goues \\
		{\tt clegoues@cs.cmu.edu}}
\date{}

\begin{document}

\maketitle

\iffalse
\textbf{TODO: do Hoare logic in this order, then SMT, then get into symbolic
  execution, etc.  Doing SMT after those things but before synthesis (as Claire
  did in 2020) is definitely sub-optimal!}
\fi

\begin{quote}
  It has been found a serious problem to define these languages [ALGOL, FORTRAN,
  COBOL] with sufficient rigor to ensure compatibility among all
  implementations...One way to achieve this would be to insist that all
  implementations of the language shall satisfy the axioms and rules of
  inference which underlie proofs of properties of programs expressed in the
  language. In effect, this is equivalent to accepting the axioms and rules of
  inference as the ultimately definitive specification of the meaning of the
  language.

\emph{C.A.R Hoare, An Axiomatic Basis for Computer Programming,1969}
\end{quote}


So far in this course we have largely represented and reasoned about programs
(and analysis of those programs) in terms of operational semantics, which gives
meaning to programs based on what happens when we execute them.  Now, we turn
our attention to a different kind of representation, which in turn enables a
different kind of static reasoning about program correctness. 

\section{Axiomatic Semantics}

Axiomatic semantics (or Hoare-style logic) defines the meaning of a
statement in terms of its effects on assertions of truth that can be made about
the associated program. This provides a formal system for reasoning about correctness.
An axiomatic semantics fundamentally consists of: (1) a language for stating
assertions about programs (where an assertion is something like ``if this
function terminates, $x > 0$ upon termination''), coupled with (2) rules for
establishing the truth of assertions.  Various logics have been used to encode
such assertions; for simplicity, we will begin by focusing on first-order logic.

In this system, a \emph{Hoare Triple} encodes such assertions:

\[
\{P\} ~S~ \{Q\}
\]

$P$ is the precondition, $Q$ is the postcondition, and $S$ is a
piece of code of interest.  Relating this back to our earlier understanding of
program semantics, this can be read as ``if $P$ holds in some state $E$ and if
$\langle S, E \rangle \Downarrow E'$, then $Q$ holds in $E'$.''  We distinguish
between partial ($\{P\}~S~\{Q\}$) and total ($[P]~S~[Q]$) correctness by saying that total
correctness means that, given precondition $P$, $S$ will terminate, and $Q$
will hold; partial correctness does not make termination guarantees.  We primarily
focus on partial correctness.

\subsection{Assertion judgements using operational semantics}

Consider a simple assertion language adding first-order predicate logic to
\WhileLang expressions:

\[
\begin{array}{lllllll}
A & \bnfdef & \mbox{true} & \bnfalt \mbox{false} & \bnfalt e_1 = e_2 & \bnfalt e_1 \ge e_2 & \bnfalt A_1 \wedge A_2 \\
  & \bnfalt & A_1 \vee A_2 & \bnfalt A_1 \implies A_2 & \bnfalt \forall x . A & \bnfalt \exists x . A
\end{array}
\]

Note that we are somewhat sloppy in mixing logical variables and program
variables; all \WhileLang variables implicitly range over integers, and all \WhileLang
boolean expressions are also assertions.

We now define an assertion judgement $E \vDash A$ , read ``$A$ is true in $E$''.
The $\vDash$ judgment is defined inductively on the structure of assertions, and
relies on the operational semantics of \WhileLang arithmetic expressions.  For
example:

\[
\begin{array}{ll}
E \vDash \mbox{true} & \mbox{\emph{always}} \\
E \vDash e_1 = e_2 & \mbox{\emph{iff}}~\langle e_1, E \rangle \Downarrow n = \langle e_2, E \rangle \Downarrow n \\
E \vDash e_1 \ge e_2 & \mbox{\emph{iff}}~ \langle e_1, E \rangle \Downarrow n \ge \langle e_2, E \rangle \Downarrow n \\
E \vDash A_1 \wedge A_2 & \mbox{\emph{iff}}~ E \vDash A_1 \text{~and~} E \vDash A_2 \\
... \\
E \vDash \forall x. A & \mbox{\emph{iff}}~ \forall n \in \Integer  . E [x := n ] \vDash A \\
E \vDash \exists x. A &  \mbox{\emph{iff}}~ \exists n \in \Integer . E[x:=n] \vDash A \\
\end{array}
\]



Now we can define formally the meaning of a partial correctness assertion
$\vDash \{ P \} ~ S ~ \{ Q \}$:

\begin{center}
$\forall E \in \mathcal{E} . \forall E' \in \mathcal{E} . ( E \vDash P \wedge
\langle S, E \rangle \Downarrow E' ) \implies E' \vDash Q$
\end{center}

\noindent Question: \textit{What about total correctness?}
\iffalse
\vspace{1em}

ANSWER: 

 $ \forall E \in \mathcal{E} . E \vDash A \implies \exists E' \in \mathcal{E}
 . \langle S, E \rangle \Downarrow E' \wedge E' \vDash B$

 or even better:

 $\forall E \in \mathcal{E} \forall E' \in \mathcal{E} . ( E \vDash A \wedge
 \langle S, E \rangle \Downarrow E') \implies E' \vDash B \wedge \forall E \in
 \mathcal{E} . E \vDash A \implies \exists E' \in \mathcal{E}. \langle S, E
 \rangle \Downarrow E'$

\textbf{WORKSHEET EXAMPLES} 
% Examples for the worksheet?
% {true } x := 5 {      }
% {         } x : = x + 3 { x = y + 3 }
% {         } x := x * 2 + 3 { x > 1 }
% { x = a} if (x < 0} then x := -x {     }
% { false } x := 3 {        }
% { x <  0} while (x != 0) x := x â€“ 1 {      }
\fi

This gives us a formal, but unsatisfactory, mechanism to decide $\vDash \{ P
\}~S~\{ Q \}$.  By defining the judgement in terms of the operational semantics,
we practically have to run the program to verify an assertion! It's also
awkward/impossible to effectively verify the truth of a $\forall x. A$ assertion
(check every integer?!).  This motivates a new symbolic technique for deriving
valid assertions from others that are known to be valid.

\subsection{Derivation rules for Hoare triples}

We write $\vdash A$ (read ``we can prove A'') when $A$ can be derived from basic
axioms. The derivation rules for $\vdash A$ are the usual ones from first-order
logic with arithmetic, like (but obviously not limited to):

\[
\infer[and]{\vdash A \wedge B}{\vdash A & \vdash B}
\]

We can now write $\vdash \{P\} ~ S ~ \{Q\}$ when we can derive a triple
using derivation rules.  There is one derivation rule for each statement type in
the language (sound familiar?):

\begin{center}
\begin{IEEEeqnarraybox}{s?s}
\infer[skip]{\vdash \{ P \}~ \mbox{skip}~ \{ P \} }{} &
\infer[assign]{\vdash \{ [e/x]P \}~ x \mbox{:=} e ~ \{ P \} }{}  \\
\end{IEEEeqnarraybox}
\end{center}

\begin{center}
\begin{IEEEeqnarraybox}{s?s}
\infer[seq]{\vdash \{P \}~ S_1;~S_2 ~ \{ Q \} }{ \vdash \{ P \}~ S_1~ \{P'\} \qquad \qquad \vdash \{ P' \}~ S_2~ \{ Q \}} &
\infer[if]{\vdash \{ P \}~ \mbox{if}~b~\mbox{then}~S_1~\mbox{else}~S_2~ \{Q \}}{ \vdash \{P \wedge b \} S_1 \{Q \} \qquad \qquad \vdash \{ P \wedge \neg b \}~ S_2~ \{ Q \}} \\
\end{IEEEeqnarraybox}
\end{center}

\noindent Question: \emph{What can we do for while?}

\vspace{1em}
There is also the \emph{rule of consequence}:

\[
\infer[consq]{\vdash \{ P' \}~ S~ \{Q' \}}{\vdash P' \implies P \qquad\qquad \vdash \{ P \}~ S~ \{ Q \} \qquad\qquad \vdash Q \implies Q' } \\
\]

This rule is important because it lets us make progress even when the pre/post
conditions in our program don't exactly match what we need (even
if they're logically equivalent) or are stronger or weaker logically than ideal.

We can use this system to prove that triples hold.  Consider $\{ \mbox{true} \}~ x := e~
\{ x = e \}$, using (in this case) the assignment rule plus the rule of
consequence:

\begin{center}
\[
\infer{\vdash \{\mbox{true} \} x := e \{ x = e \}}{\vdash \mbox{true} \implies e = e \qquad\qquad {\infer{\{ e = e \}~ x := e~\{ x = e \}}{}}}
\]
\end{center}

\looseness-1
A system of axiomatic semantics is \emph{sound} if everything we can prove is
also true, that is: $\mathtt{if}~\vdash \{ P \} S \{ Q \}~\mathtt{then}~\vDash \{ P \} S
\{ Q \}$.  
This can be proven via simultaneous induction on the structure of the
operational semantics derivation and the axiomatic semantics proof; will not
conduct this proof in these notes.  Intuitively, it
expresses that the axiomatic proof we can derive using these rules is equivalent
to the operational semantics derivation

A system of axiomatic semantics is \emph{complete} if we can prove all true things: 
 $\mathtt{if}~\vDash \{ P \} S \{ Q \} ~ \mathtt{then}~\vdash \{ P \} S \{ Q \}
 $
%
The system we have outlined is relatively complete (that is, as complete as the
underlying logic). We now move to showing how to (soundly/completely) prove
properties of programs using this style of semantics.

\section{Proofs of a Program}

Hoare-style verification is based on the idea of a
specification as a contract between the implementation of a function and its
clients.  The specification consists of the precondition and a postcondition.
The precondition is a predicate describing the condition the code/function relies on
for correct operation; the client must fulfill this condition.  The
postcondition is a predicate describing the condition the function establishes
after correctly running; the client can rely on this condition being true after
the call to the function.

Note that if a client calls a function without fulfilling its precondition, the
function can behave in any way at all and still be correct.  Therefore, if
a function must be robust to errors, the precondition should include
the possibility of erroneous input, and the postcondition should describe what
should happen in case of that input (e.g. an exception being thrown).

The goal in Hoare-style verification is thus to (statically!) prove that, given
a pre-condition, a particular post-condition will hold after a piece of code
executes.  We do this by generating a logical formula known as a
\emph{verification condition}, constructed such that, if true, we know that the program
behaves as specified.  The general strategy for doing this, introduced by
Dijkstra, relies on the idea of a \emph{weakest precondition} of a statement
with respect to the desired post-condition.  We then show that the
given precondition implies it.  However, loops, as ever, complicate this
strategy.

\subsection{Strongest postconditions and weakest pre-conditions}

We can write any number of perfectly valid Hoare triples.  Consider the Hoare
triple $\{x = 5\} ~x := x * 2~ \{ x > 0 \}$.  This triple is clearly correct,
because if $x=5$ and we multiply $x$ by $2$, we get $x=10$ which clearly implies
that $x>0$.  However, although correct, this Hoare triple is not a precise as we
might like.  Specifically, we could write a stronger postcondition, i.e. one
that implies $x>0$.  For example, $x>5 \land x < 20$ is stronger because it is
more informative; it pins down the value of $x$ more precisely than $x>0$.  The
strongest postcondition possible is $x=10$; this is the most useful
postcondition.  Formally, if $\{P\} ~S~ \{Q\}$ and for all $Q'$ such that
$\{P\} ~S~ \{Q'\}$, $Q \Rightarrow Q'$, then $Q$ is the strongest
postcondition of $S$ with respect to $P$.

We can compute the strongest postcondition for a given statement and
precondition using the function $sp(S, P)$.  Consider the case of a statement of
the form $x := E$.  If the condition $P$ held before the statement, we now know
that $P$ still holds and that $x=E$---where $P$ and $E$ are now in terms of the
old, pre-assigned value of $x$.  For example, if $P$ is $x + y = 5$, and $S$ is
$x := x + z$, then we should know that $x' + y = 5$ and $x = x' + z$, where $x'$
is the old value of x.  The program semantics doesn't keep track of the old
value of $x$, but we can express it by introducing a fresh, existentially
quantified variable $x'$.  This gives us the following strongest postcondition
for assignment:%
\footnote{Recall that the operation $[x'/x]E$ denotes the capture-avoiding substitution of
$x'$ for $x$ in $E$; we rename bound variables as we do the substitution so as to
avoid conflicts.}

\[
\begin{array}{ll}
sp(x := E, P) & = \exists x' . [x'/x]P \land x = [x'/x]E
\end{array}
\]


% But is hard, so instead reason using weakest preconditions
While this scheme is workable, it is awkward to existentially quantify over a
fresh variable at every statement; the formulas produced become unnecessarily
complicated, and if we want to use automated theorem provers, the additional
quantification tends to cause problems.  Dijkstra proposed reasoning instead in
terms of \textit{weakest preconditions}, which turns out to work better.
If $\{P\} ~S~ \{Q\}$ and for all $P'$ such that $\{P'\} ~S~ \{Q\}$, $P'
\Rightarrow P$, then $P$ is the weakest precondition $wp(S,Q)$ of $S$ with
respect to $Q$.

\iffalse \textbf{WORKSHEET EXAMPLES} \fi

%Exercise:
% y=1 {z=y+1} x:=z*2 {x=4}
% y>2 {y=7} x:=y+3 {x>5}
% y!=0 {false} x:=2/y {true}
% {y<16} x:=2/y {x<8}
% Which of the Hoare triples above are valid?
% For which of the valid triples
% Can you write a stronger postcondition? Leave the precondition unchanged, and ensure the resulting triple is still valid.
% Can you write a weaker precondition? Leave the postcondition unchanged, and ensure the resulting triple is still valid.

We can define a function yielding the weakest precondition inductively,
following the Hoare rules.  For for assignments, sequences, and if
statements, this yields:

\[
\begin{array}{ll}
wp(x := E, P) & = [E/x] P
\\[1ex]
wp(S;~ T, Q) & = wp(S, ~wp(T, Q))
\\[1ex]
wp(\mbox{if}~ B ~\mbox{then}~ S ~\mbox{else}~ T, Q) & = B \Rightarrow wp(S,Q) \land \lnot B \Rightarrow wp(T,Q)
\end{array}
\]
\iffalse
\textbf{PROBABLY DON'T HAVE TIME:}
 exercise:
 Fill in the missing pre- or post-conditions with predicates that make each Hoare triple valid.
 {x=y} x:=y*2 {      }
 {      } x:=x+3{x=z}
 {       } x:=x+1;y:=y*x{y=2*z}
 {       } if (x>0) then y:=x else y:=0 {y>0}
\fi

\subsection{Loops}

As usual, things get tricky when we get to loops.  Consider:

\begin{center}
\[
\{P\}~\mbox{while} (i<x)~ \mbox{do}~f=f*i; i:=i+1~~ \mbox{done} \{f=x!\}
\]
\end{center}

\looseness-1
What is the weakest precondition here?  Fundamentally, we
need to prove by induction that the property we care about will generalize
across an arbitrary number of loop iterations.  Thus, $P$ is the base case,
and we need some inductive hypothesis that is preserved when executing loop body
an arbitrary number of times. We commonly refer to this hypothesis as a \textit{loop
  invariant}, because it represents a condition that is always true
(i.e. invariant) before and after each execution of the loop.

Computing weakest preconditions on loops is very difficult on real languages.
Instead, we assume the provision of that loop invariant.  A loop invariant must
fulfill the following conditions:

\begin{itemize}
\item $P \Rightarrow I$ : The invariant is initially true.  This condition is
  necessary as a base case, to establish the induction hypothesis.

\item $\{ Inv \land B \} ~S~ \{Inv\}$ : Each execution of the loop preserves the
  invariant.  This is the inductive case of the proof.

\item $(Inv \land \lnot B) \Rightarrow Q$ : The invariant and the loop exit
  condition imply the postcondition.  This condition is simply demonstrating
  that the induction hypothesis/loop invariant we have chosen is sufficiently
  strong to prove our postcondition $Q$.
\end{itemize}

The procedure outlined above only verifies partial correctness, because it does
not reason about how many times the loop may execute.  Verifying full
correctness involves placing an upper bound on the number of remaining times the
loop body will execute, typically called a \textit{variant function}, written
$v$, because it is variant: we must prove that it decreases each time we go
through the loop.  We mention this for the interested reader; we will not spend
much time on it.

\subsection{Proving programs}

Assume a version of \WhileLang that annotates loops with invariants:
$\mbox{while}_{inv}~b~\mbox{do}~S$.
%
Given such a program, and associated pre- and post-conditions:

\[
\{ P \} ~ S_{inv}~ \{ Q \}
\]

The proof strategy constructs a verification condition $VC(S_{annot}, Q)$ that
we seek to prove true (usually with the help of a theorem prover).  $VC$ is
guaranteed to be stronger than $wp(S_{annot}, Q)$ but still weaker than $P$: $P
\implies VC(S_{annot}, Q) \implies wp(S_{annot}, Q)$ We compute $VC$ using a
verification condition generation procedure $VCGen$, which mostly follows the
definition of the $wp$ function discussed above:

\begin{center}
\[
\begin{array}{lcl}
VCGen(\mbox{skip}, Q) & = & Q \\
VCGen(S_1; S_2, Q) & = & VCGen(S_1, VCGen(S_2, Q)) \\
VCGen(\mbox{if}~b~\mbox{then}~S_1~\mbox{else}~S_2, Q) & = & b \implies VCGen(S_1, Q) \wedge \neg b \implies VCGen(S_2, Q) \\
VCGen(x := e, Q) & = & [e/x] Q \\
\end{array}
\]
\end{center}

% what about let?

The one major point of difference is in the handling of loops:

\begin{center}
\[
\begin{array}{l}
VCGen(\mbox{while}_{inv} ~e~\mbox{do}~S, Q) = Inv \wedge (\forall x_1 ... x_n . Inv \implies ( e \implies VCGen(S, Inv) \wedge \neg e \implies Q) )
\end{array}
\]
\end{center}

% first part: inv holds on entry
% second part is preserve d in parbitrary interation; like in induction
% Q holds when loop terminates in arbitrary iteration.

To see this in action, consider the following \WhileLang program:

\[
\begin{array}{l}
r := 1;\\
i := 0;\\
\pwhile~ i<m ~\pdo \\
\hspace{1cm} r:=r*n; \\
\hspace{1cm} i:=i+1
\end{array}
\]

We wish to prove that this function computes the $n$th power of $m$
and leaves the result in $r$.  We can state this with the
postcondition $r=n^m$.

Next, we need to determine a precondition for the program.  We cannot simply
compute it with $wp$ because we do not yet know the loop
invariant is---and in fact, different loop invariants could lead to
different preconditions.  However, a bit of reasoning will help.  We
must have $m \ge 0$ because we have no provision for dividing by $n$,
and we avoid the problematic computation of $0^0$ by assuming $n>0$.
Thus our precondition will be $m \ge 0 \land n > 0$.

A good heuristic for choosing a
loop invariant is often to modify the postcondition of the loop to
make it depend on the loop index instead of some other variable.
Since the loop index runs from $i$ to $m$, we can guess that we should
replace $m$ with $i$ in the postcondition $r=n^m$.  This gives us a
first guess that the loop invariant should include $r=n^i$.

This loop invariant is not strong enough, however, because the loop invariant conjoined with the
loop exit condition should imply the postcondition.  The loop exit
condition is $i \ge m$, but we need to know that $i = m$.  We can get
this if we add $i \le m$ to the loop invariant.  In addition, for
proving the loop body correct, we will also need to add $0 \le i$ and
$n > 0$ to the loop invariant.  Thus our full loop invariant
will be $r=n^i \land 0 \le i \le m \land n > 0$.

Our next task is to use weakest preconditions to generate proof
obligations that will verify the correctness of the specification.
We will first ensure that the invariant is initially true when
the loop is reached, by propagating that invariant past the first
two statements in the program:

\[
\begin{array}{l}
\{m \ge 0 \land n > 0\}\\
r := 1;\\
i := 0;\\
\{ r=n^i \land 0 \le i \le m \land n > 0 \}
\end{array}
\]

We propagate the loop invariant past $i :=0$ to get
$r=n^0 \land 0 \le 0 \le m \land n > 0$.  We propagate this
past $r :=1$ to get $1=n^0 \land 0 \le 0 \le m \land n > 0$.
Thus our proof obligation is to show that:

\[
\begin{array}{l}
m \ge 0 \land n > 0
\Rightarrow 1=n^0 \land 0 \le 0 \le m \land n > 0
\end{array}
\]

We prove this with the following logic:

\[
\begin{array}{ll}
m \ge 0 \land n > 0 & \mbox{by assumption}\\
1=n^0 & \mbox{because $n^0=1$ for all $n>0$ and we know $n>0$}\\
0 \le 0 & \mbox{by definition of $\le$}\\
0 \le m & \mbox{because $m \ge 0$ by assumption}\\
n > 0 & \mbox{by the assumption above}\\
1=n^0 \land 0 \le 0 \le m \land n > 0 & \mbox{by conjunction of the above}\\
\end{array}
\]

To show the loop invariant is preserved, we have:

\[
\begin{array}{l}
\{ r=n^i \land 0 \le i \le m \land n > 0  \land i < m\}\\
r := r*n;\\
i := i+1;\\
\{ r=n^i \land 0 \le i \le m \land n > 0 \}
\end{array}
\]

We propagate the invariant past $i:=i+1$ to get $r=n^{i+1} \land 0 \le i+1 \le m
\land n > 0$.  We propagate this past $r:=r*n$ to get: $r*n=n^{i+1} \land 0 \le
i+1 \le m \land n > 0$.  Our proof obligation is therefore:

\[
\begin{array}{l}
r=n^i \land 0 \le i \le m \land n > 0  \land i < m\\
\Rightarrow r*n=n^{i+1} \land 0 \le i+1 \le m \land n > 0
\end{array}
\]

We can prove this as follows:

\[
\begin{array}{ll}
r=n^i \land 0 \le i \le m \land n > 0  \land i < m & \mbox{by assumption}\\
r*n=n^i*n & \mbox{multiplying by $n$}\\
r*n=n^{i+1} & \mbox{definition of exponentiation}\\
0 \le i+1 & \mbox{because $0 \le i$}\\
i+1 < m+1 & \mbox{by adding 1 to inequality}\\
i+1 \le m & \mbox{by definition of $\le$}\\
n>0 & \mbox{by assumption}\\
r*n=n^{i+1} \land 0 \le i+1 \le m \land n > 0& \mbox{by conjunction of the above}\\
\end{array}
\]

Last, we need to prove that the postcondition holds when we exit the
loop.  We have already hinted at why this will be so when we chose the
loop invariant.  However, we can state the proof obligation formally:

\[
\begin{array}{l}
r=n^i \land 0 \le i \le m \land n > 0  \land i \ge m\\
\Rightarrow r=n^m
\end{array}
\]

We can prove it as follows:

\[
\begin{array}{ll}
r=n^i \land 0 \le i \le m \land n > 0  \land i \ge m & \mbox{by assumption}\\
i = m & \mbox{because $i \le m$ and $i \ge m$}\\
r=n^m & \mbox{substituting $m$ for $i$ in assumption}\\
\end{array}
\]

% Example together: the one from slides
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
